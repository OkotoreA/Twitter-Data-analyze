{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "name": "EURO TWITTER SENTIMENT ANALYSIS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc16c030"
      },
      "source": [
        "# TWITTER SENTIMENT ANALYSIS\n",
        "#### This notebook walks through the basics of sentiment analysis using Tweets from the "
      ],
      "id": "cc16c030"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "634877db"
      },
      "source": [
        "# import the pandas library to read the dataset\n",
        "import pandas as pd"
      ],
      "id": "634877db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy\n",
        "!pip install snscrape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAr1Yjxue93r",
        "outputId": "03f06498-8499-4ae7-924a-99f9bbc76e1e"
      },
      "id": "tAr1Yjxue93r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
            "Collecting snscrape\n",
            "  Downloading snscrape-0.3.4-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.2.6)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from snscrape) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX3tauIT8t6E",
        "outputId": "a28e4ead-90b7-4c70-dcf1-16759b92e06c"
      },
      "id": "VX3tauIT8t6E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # importing libraries and packages\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas\n",
        "\n",
        "# Creating list to append tweet data \n",
        "tweets_list1 = []\n",
        "\n",
        "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:jack').get_items()): #declare a username \n",
        "    if i>1000: #number of tweets you want to scrape\n",
        "        break\n",
        "    tweets_list1.append([tweet.date, tweet.id, tweet.content]) #declare the attributes to be returned\n",
        "   \n",
        "# Creating a dataframe from the tweets list above \n",
        "tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text'])\n",
        "\n",
        "tweets_df1.dropna(axis=0, inplace = True)"
      ],
      "metadata": {
        "id": "IMOKGt_Fe5D5"
      },
      "id": "IMOKGt_Fe5D5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet"
      ],
      "metadata": {
        "id": "aeExk2EGhShA"
      },
      "id": "aeExk2EGhShA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas\n",
        "\n",
        "# Creating list to append tweet data to\n",
        "tweets_list2 = []\n",
        "\n",
        "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('COVID Vaccine since:2021-01-01 until:2021-05-31').get_items()):\n",
        "    if i>5000:\n",
        "        break\n",
        "    tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
        "    \n",
        "# Creating a dataframe from the tweets list above\n",
        "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])"
      ],
      "metadata": {
        "id": "A1b8wVaAfhiY"
      },
      "id": "A1b8wVaAfhiY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "3d184542",
        "outputId": "3f3e20db-fe37-4c4f-fdc9-e1f88c09c03c"
      },
      "source": [
        "''' Read the dataset and display the first 5 rows\n",
        "Some text in the Tweet column have characters like ™, ®, ©, and they could be turned to unwanted characters. \n",
        "encoding=\"ISO-8859-1\" fixes this'''\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Python Updated Scripts EURO.csv\", encoding=\"ISO-8859-1\")\n",
        "df.head(5)"
      ],
      "id": "3d184542",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Like count</th>\n",
              "      <th>Quoted Tweet</th>\n",
              "      <th>Reply</th>\n",
              "      <th>Retweet count</th>\n",
              "      <th>Retweeted</th>\n",
              "      <th>Username</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tweet id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7/12/2021 23:55</td>\n",
              "      <td>2.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Ryandavies_13</td>\n",
              "      <td>It is my birthday today&lt;U+0001F601&gt; #birthday ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.410000e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7/12/2021 23:55</td>\n",
              "      <td>1.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>TheTycoon2</td>\n",
              "      <td>So #EuroFinal, a penalty shootout and #England...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.410000e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7/12/2021 23:41</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Beatlebun</td>\n",
              "      <td>guess who lost the penalties yesterday #EURO20...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.410000e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7/12/2021 23:49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>cancelracismnow</td>\n",
              "      <td>@jadjaya @chiellini The number of times I said...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.410000e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7/12/2021 23:58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>JRF1875</td>\n",
              "      <td>@GaryLineker @England You're a bit late with t...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.410000e+18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Datetime  Like count  ... Sentiment      Tweet id\n",
              "0  7/12/2021 23:55         2.0  ...       1.0  1.410000e+18\n",
              "1  7/12/2021 23:55         1.0  ...       1.0  1.410000e+18\n",
              "2  7/12/2021 23:41         0.0  ...       1.0  1.410000e+18\n",
              "3  7/12/2021 23:49         0.0  ...       1.0  1.410000e+18\n",
              "4  7/12/2021 23:58         0.0  ...       0.0  1.410000e+18\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "ebe69aa5",
        "outputId": "a21284bf-a0dd-4829-93de-aca1cd20e437"
      },
      "source": [
        "# Read the first 1000 rows\n",
        "df = df.iloc[:999]\n",
        "df.tail()"
      ],
      "id": "ebe69aa5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Like count</th>\n",
              "      <th>Quoted Tweet</th>\n",
              "      <th>Reply</th>\n",
              "      <th>Retweet count</th>\n",
              "      <th>Retweeted</th>\n",
              "      <th>Username</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tweet id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>7/12/2021 19:38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>n00byz</td>\n",
              "      <td>@Savills made it on #channel4news       #Euro2...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.410000e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>7/12/2021 19:37</td>\n",
              "      <td>31.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>10.0</td>\n",
              "      <td>No</td>\n",
              "      <td>teh_jimzor</td>\n",
              "      <td>\"If you abuse anyone on social media, you're n...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.410000e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>7/12/2021 19:39</td>\n",
              "      <td>1.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>1.0</td>\n",
              "      <td>No</td>\n",
              "      <td>callumowennn17</td>\n",
              "      <td>#EURO2020 #ITA vs #ENG - twitter reacts to #Eu...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.410000e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>7/12/2021 19:37</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>LisaS_1981</td>\n",
              "      <td>Saka, just keep your head high and please igno...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.410000e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>7/12/2021 19:37</td>\n",
              "      <td>8.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>7.0</td>\n",
              "      <td>No</td>\n",
              "      <td>fionaboothHT</td>\n",
              "      <td>I shared your tweet at the end of Collective W...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.410000e+18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Datetime  Like count  ... Sentiment      Tweet id\n",
              "994  7/12/2021 19:38         0.0  ...       1.0  1.410000e+18\n",
              "995  7/12/2021 19:37        31.0  ...       1.0  1.410000e+18\n",
              "996  7/12/2021 19:39         1.0  ...       1.0  1.410000e+18\n",
              "997  7/12/2021 19:37         3.0  ...       2.0  1.410000e+18\n",
              "998  7/12/2021 19:37         8.0  ...       2.0  1.410000e+18\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6b50827"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "The columns needed for this sentiment analysis are \"Tweet\" and \"Sentiment\".\n",
        "\n",
        "The Sentiment column was hardcoded in Excel to train the model to recognize the sentiment associated with each tweet. \n",
        "0-Negative\n",
        "1-Neutral\n",
        "2-Positive"
      ],
      "id": "c6b50827"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00a812f8"
      },
      "source": [
        "# Extract the major parameters\n",
        "x = df[\"Tweet\"]\n",
        "y = df[\"Sentiment\"]"
      ],
      "id": "00a812f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c751980"
      },
      "source": [
        ""
      ],
      "id": "9c751980",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d45d0d4"
      },
      "source": [
        "The next step is to remove punctuations, hashtags, and stopwords like a, the, an, etc that do not affect the meaning of the tweets.\n"
      ],
      "id": "0d45d0d4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4925b148"
      },
      "source": [
        "import nltk"
      ],
      "id": "4925b148",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3abff196",
        "outputId": "ecfd31f6-5250-453a-a791-2919a28cffb2"
      },
      "source": [
        "# import libraries to help with preprocessing\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer"
      ],
      "id": "3abff196",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ba7b5e0"
      },
      "source": [
        "# This removes the stopwords in English Language\n",
        "stop_words=stopwords.words('english')\n",
        "stemmer=PorterStemmer()"
      ],
      "id": "1ba7b5e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae6db1b9"
      },
      "source": [
        "# Next, we remove all special characters, single letters and convert them to lower case\n",
        "\n",
        "import re\n",
        "cleaned_data=[]  # a list to store all cleaned tweets\n",
        "for i in range(len(x)):  # interates through every tweet\n",
        "    \n",
        "    tweet=re.sub('[^a-zA-Z]', ' ', x.iloc[i])  # removes all special characters\n",
        "    tweet=re.sub(r'\\s+[a-zA-Z]\\s+', ' ', tweet)  # removes all single letters \n",
        "    tweet=tweet.lower().split()  # turns all text to lower case\n",
        "    \n",
        "    tweet=[stemmer.stem(word) for word in tweet if (word not in stop_words)]  # removes all stop words\n",
        "    tweet=' '.join(tweet)  # joins the words to make a sentence\n",
        "    cleaned_data.append(tweet) # appends all individual sentences to form a list "
      ],
      "id": "ae6db1b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce517fe0"
      },
      "source": [
        "print(cleaned_data)"
      ],
      "id": "ce517fe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R_ZTbDP1O6S"
      },
      "source": [
        ""
      ],
      "id": "4R_ZTbDP1O6S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdCnrxDv1PIl"
      },
      "source": [
        "## Bag of Words\n",
        "Bag of words simplifies representation used in natural language processing. It creates a matrix table, where each row represents a sentence and each word will have separate column for itself that represents it’s frequency.\n"
      ],
      "id": "CdCnrxDv1PIl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3863a9be"
      },
      "source": [
        "'''The Count Vectorizer function converts a list of words into bag of words\n",
        "max_features is set to 3000 which means, only 3000 of the most occurring words are used to create a bag of words\n",
        "stop_words is used to remove words that frequently appear in the dataset which have no sentiment'''\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=200, stop_words=[\"https\", \"euro\", \"final\", \"england\"])\n",
        "bag_of_words = cv.fit_transform(cleaned_data).toarray()"
      ],
      "id": "3863a9be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T-d3pperb4QP"
      },
      "id": "T-d3pperb4QP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_bKOMsru8JM"
      },
      "source": [
        "print(bag_of_words)"
      ],
      "id": "a_bKOMsru8JM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prqu-9dK4aO9"
      },
      "source": [
        "## Training the Model\n",
        "Multinomial Naive Bayes model is used to build the NLP model using the input(x = df[\"Tweet\"], now bag_of_words) and output(y = df[\"Sentiment\"])"
      ],
      "id": "prqu-9dK4aO9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hwvtjYfvNNy"
      },
      "source": [
        "# Import Multinomial Naive Bayes model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model=MultinomialNB()"
      ],
      "id": "5hwvtjYfvNNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn0hmxxxvTNR",
        "outputId": "a4dfd393-ed2f-422a-99da-a7d2f6a0ea78"
      },
      "source": [
        "# Split the dataset into train and test, then train the model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(bag_of_words, y, test_size=0.3) # test size is 30% of the data \n",
        "model.fit(X_train, y_train)"
      ],
      "id": "cn0hmxxxvTNR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbJy352Y58kM"
      },
      "source": [
        ""
      ],
      "id": "ZbJy352Y58kM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaMkYsldvgiN",
        "outputId": "645b750b-ee01-4c84-85b3-df52f9187d4e"
      },
      "source": [
        "# Check the accuracy using classification_report from sklearn under important parameters like precision, recall, f1 score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "from sklearn.metrics import classification_report\n",
        "cf=classification_report(y_test,y_pred)\n",
        "print(cf)"
      ],
      "id": "OaMkYsldvgiN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.42      0.29      0.34        38\n",
            "         1.0       0.79      0.86      0.82       207\n",
            "         2.0       0.51      0.45      0.48        55\n",
            "\n",
            "    accuracy                           0.71       300\n",
            "   macro avg       0.57      0.53      0.55       300\n",
            "weighted avg       0.69      0.71      0.70       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j77aerhjNCQB"
      },
      "source": [
        "Accuracy of the model is 68% "
      ],
      "id": "j77aerhjNCQB"
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.pkl'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "F70bGPBHG3C5"
      },
      "id": "F70bGPBHG3C5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read the data\n",
        "# Extract the major parameters\n",
        "x = tweets_df1[\"Text\"]\n",
        "\n",
        "import nltk\n",
        "\n",
        "# import libraries to help with preprocessing\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# This removes the stopwords in English Language\n",
        "stop_words=stopwords.words('english')\n",
        "stemmer=PorterStemmer()\n",
        "\n",
        "# Next, we remove all special characters, single letters and convert them to lower case\n",
        "\n",
        "import re\n",
        "cleaned_data=[]  # a list to store all cleaned tweets\n",
        "for i in range(len(x)):  # interates through every tweet\n",
        "    \n",
        "    tweet=re.sub('[^a-zA-Z]', ' ', x.iloc[i])  # removes all special characters\n",
        "    tweet=re.sub(r'\\s+[a-zA-Z]\\s+', ' ', tweet)  # removes all single letters \n",
        "    tweet=tweet.lower().split()  # turns all text to lower case\n",
        "    \n",
        "    tweet=[stemmer.stem(word) for word in tweet if (word not in stop_words)]  # removes all stop words\n",
        "    tweet=' '.join(tweet)  # joins the words to make a sentence\n",
        "    cleaned_data.append(tweet) # appends all individual sentences to form a list \n",
        "\n",
        "\n",
        "\n",
        "'''The Count Vectorizer function converts a list of words into bag of words\n",
        "max_features is set to 3000 which means, only 3000 of the most occurring words are used to create a bag of words\n",
        "stop_words is used to remove words that frequently appear in the dataset which have no sentiment'''\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=200, stop_words=[\"https\", \"euro\", \"final\", \"england\"])\n",
        "bag_of_words = cv.fit_transform(cleaned_data).toarray()\n",
        "\n",
        "\n",
        "y_pred = model.predict(bag_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpQ5Ayy-HPc8",
        "outputId": "37fd1032-1c75-4b87-fa6c-de2741878245"
      },
      "id": "YpQ5Ayy-HPc8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df1[\"Sentiment\"] = y_pred"
      ],
      "metadata": {
        "id": "AlatbGFhcSQO"
      },
      "id": "AlatbGFhcSQO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df1.Sentiment.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpxFBsF2cxvr",
        "outputId": "17d7cae9-d978-4c61-bb8d-0ec4ba312bab"
      },
      "id": "MpxFBsF2cxvr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}