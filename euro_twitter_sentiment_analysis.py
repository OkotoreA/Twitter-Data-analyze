# -*- coding: utf-8 -*-
"""EURO TWITTER SENTIMENT ANALYSIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jmrTwbLnDAINrEQ-GJctLO71hEfXCyKQ

# TWITTER SENTIMENT ANALYSIS
#### This notebook walks through the basics of sentiment analysis using Tweets from the
"""

# import the pandas library to read the dataset
import pandas as pd

!pip install tweepy
!pip install snscrape

from google.colab import drive
drive.mount('/content/drive')

# importing libraries and packages
import snscrape.modules.twitter as sntwitter
import pandas

# Creating list to append tweet data 
tweets_list1 = []

# Using TwitterSearchScraper to scrape data and append tweets to list
for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:jack').get_items()): #declare a username 
    if i>1000: #number of tweets you want to scrape
        break
    tweets_list1.append([tweet.date, tweet.id, tweet.content]) #declare the attributes to be returned
   
# Creating a dataframe from the tweets list above 
tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text'])

tweets_df1.dropna(axis=0, inplace = True)

tweet

import snscrape.modules.twitter as sntwitter
import pandas

# Creating list to append tweet data to
tweets_list2 = []

# Using TwitterSearchScraper to scrape data and append tweets to list
for i,tweet in enumerate(sntwitter.TwitterSearchScraper('COVID Vaccine since:2021-01-01 until:2021-05-31').get_items()):
    if i>5000:
        break
    tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])
    
# Creating a dataframe from the tweets list above
tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])

''' Read the dataset and display the first 5 rows
Some text in the Tweet column have characters like ™, ®, ©, and they could be turned to unwanted characters. 
encoding="ISO-8859-1" fixes this'''
import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/Python Updated Scripts EURO.csv", encoding="ISO-8859-1")
df.head(5)

# Read the first 1000 rows
df = df.iloc[:999]
df.tail()

"""## Data preprocessing

The columns needed for this sentiment analysis are "Tweet" and "Sentiment".

The Sentiment column was hardcoded in Excel to train the model to recognize the sentiment associated with each tweet. 
0-Negative
1-Neutral
2-Positive
"""

# Extract the major parameters
x = df["Tweet"]
y = df["Sentiment"]



"""The next step is to remove punctuations, hashtags, and stopwords like a, the, an, etc that do not affect the meaning of the tweets.

"""

import nltk

# import libraries to help with preprocessing
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.stem import PorterStemmer

# This removes the stopwords in English Language
stop_words=stopwords.words('english')
stemmer=PorterStemmer()

# Next, we remove all special characters, single letters and convert them to lower case

import re
cleaned_data=[]  # a list to store all cleaned tweets
for i in range(len(x)):  # interates through every tweet
    
    tweet=re.sub('[^a-zA-Z]', ' ', x.iloc[i])  # removes all special characters
    tweet=re.sub(r'\s+[a-zA-Z]\s+', ' ', tweet)  # removes all single letters 
    tweet=tweet.lower().split()  # turns all text to lower case
    
    tweet=[stemmer.stem(word) for word in tweet if (word not in stop_words)]  # removes all stop words
    tweet=' '.join(tweet)  # joins the words to make a sentence
    cleaned_data.append(tweet) # appends all individual sentences to form a list

print(cleaned_data)



"""## Bag of Words
Bag of words simplifies representation used in natural language processing. It creates a matrix table, where each row represents a sentence and each word will have separate column for itself that represents it’s frequency.

"""

'''The Count Vectorizer function converts a list of words into bag of words
max_features is set to 3000 which means, only 3000 of the most occurring words are used to create a bag of words
stop_words is used to remove words that frequently appear in the dataset which have no sentiment'''

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=200, stop_words=["https", "euro", "final", "england"])
bag_of_words = cv.fit_transform(cleaned_data).toarray()



print(bag_of_words)

"""## Training the Model
Multinomial Naive Bayes model is used to build the NLP model using the input(x = df["Tweet"], now bag_of_words) and output(y = df["Sentiment"])
"""

# Import Multinomial Naive Bayes model
from sklearn.naive_bayes import MultinomialNB
model=MultinomialNB()

# Split the dataset into train and test, then train the model

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(bag_of_words, y, test_size=0.3) # test size is 30% of the data 
model.fit(X_train, y_train)



# Check the accuracy using classification_report from sklearn under important parameters like precision, recall, f1 score

y_pred = model.predict(X_test)
from sklearn.metrics import classification_report
cf=classification_report(y_test,y_pred)
print(cf)

"""Accuracy of the model is 68% """

import pickle

# save the model to disk
filename = 'finalized_model.pkl'
pickle.dump(model, open(filename, 'wb'))

#read the data
# Extract the major parameters
x = tweets_df1["Text"]

import nltk

# import libraries to help with preprocessing
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.stem import PorterStemmer

# This removes the stopwords in English Language
stop_words=stopwords.words('english')
stemmer=PorterStemmer()

# Next, we remove all special characters, single letters and convert them to lower case

import re
cleaned_data=[]  # a list to store all cleaned tweets
for i in range(len(x)):  # interates through every tweet
    
    tweet=re.sub('[^a-zA-Z]', ' ', x.iloc[i])  # removes all special characters
    tweet=re.sub(r'\s+[a-zA-Z]\s+', ' ', tweet)  # removes all single letters 
    tweet=tweet.lower().split()  # turns all text to lower case
    
    tweet=[stemmer.stem(word) for word in tweet if (word not in stop_words)]  # removes all stop words
    tweet=' '.join(tweet)  # joins the words to make a sentence
    cleaned_data.append(tweet) # appends all individual sentences to form a list 



'''The Count Vectorizer function converts a list of words into bag of words
max_features is set to 3000 which means, only 3000 of the most occurring words are used to create a bag of words
stop_words is used to remove words that frequently appear in the dataset which have no sentiment'''

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=200, stop_words=["https", "euro", "final", "england"])
bag_of_words = cv.fit_transform(cleaned_data).toarray()


y_pred = model.predict(bag_of_words)

tweets_df1["Sentiment"] = y_pred

tweets_df1.Sentiment.unique()